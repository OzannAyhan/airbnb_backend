{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Airbnb Dasboard Project Preprocessing & NLP & Arima**"
      ],
      "metadata": {
        "id": "ZbCHqS_bck5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Git Repo Cloning**"
      ],
      "metadata": {
        "id": "8fdT6mMScfVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://airbnbdashboard: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@github.com/airbnbdashboard/airbnb_backend.git\n",
        "!pip install --upgrade tensorflow\n",
        "\n",
        "import sys\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "if 'airbnb_backend' not in sys.path:\n",
        "    sys.path.append('/content/airbnb_backend')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e98qOHbxGL0V",
        "outputId": "0372140d-c619-42cb-e5a4-a07733892805"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'airbnb_backend'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 31 (delta 13), reused 26 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 12.03 KiB | 12.03 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbByv86lnUOy"
      },
      "source": [
        "###**Installing Required Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC575b5__kDk",
        "outputId": "07223ec5-9032-4039-8710-daa4720ea78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "IYEcxfbtswku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from airbnb_backend.Arima import arima_forecast_and_save\n",
        "from airbnb_backend.Load_Data import download_and_extract_city_data\n",
        "from airbnb_backend.Merge_Listings_Calendar_Data import prepare_combined_data\n",
        "from airbnb_backend.Process_Calendar_Data import process_city_calender\n",
        "from airbnb_backend.Process_Listing_Data import process_city_listings\n",
        "from airbnb_backend.Processing_Amenities import process_combined_data\n",
        "from airbnb_backend.Sentiment_Analysis_Description import analyze_sentiment\n",
        "from airbnb_backend.Sentiment_Analysis_Reviews import process_city_reviews\n",
        "from airbnb_backend.Zero_Shot_Classification import classify_property_descriptions"
      ],
      "metadata": {
        "id": "sNYrvtutQV4j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dl6k7HUxjZbn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Define the City Name to be Processed**\n",
        "\n",
        "When you define the name and run all the following funtions, you will be able to get the final data of the defined data. Please use lowercase leeter.\n",
        "\n",
        "Options are:\n",
        "(barcelona, madrid, florence, mallorca, lisbon, milan, rome)"
      ],
      "metadata": {
        "id": "z-ddx9IVdGzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the city name\n",
        "city_name = \"mallorca\""
      ],
      "metadata": {
        "id": "69hmojcW2pE6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Download and Extract Files from Google Drive**\n",
        "\n",
        "This function downloads and extracts data for a pre-selected city from Google Drive:\n",
        "\n",
        "- **File Retrieval**: Uses the pre-defined city name to get the corresponding Google Drive file ID.\n",
        "- **Download**: Downloads the zip file directly into memory.\n",
        "- **Extraction**: Extracts the contents to the `/content` directory.\n",
        "\n"
      ],
      "metadata": {
        "id": "YrdzXqFwpY8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract_city_data(city_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgwvinT0Yjn2",
        "outputId": "c0aa22c6-13db-4c4f-9f4f-007542b559fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1wcaxi6wWu2G7MQLf0u0WRFggVNgnAQ0y\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1wcaxi6wWu2G7MQLf0u0WRFggVNgnAQ0y&confirm=t&uuid=93057ac1-8355-4bcb-9e89-654adb9ab70a\n",
            "To: <_io.BytesIO object at 0x7c91c3ba02c0>\n",
            "100%|██████████| 336M/336M [00:01<00:00, 204MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the zip for mallorca: ['calendar_mallorca2.csv', '__MACOSX/._calendar_mallorca2.csv', 'calendar_mallorca1.csv', '__MACOSX/._calendar_mallorca1.csv', 'mallorca_reviews4.csv', '__MACOSX/._mallorca_reviews4.csv', 'mallorca_reviews3.csv', '__MACOSX/._mallorca_reviews3.csv', 'mallorca_reviews2.csv', '__MACOSX/._mallorca_reviews2.csv', 'mallorca_reviews1.csv', '__MACOSX/._mallorca_reviews1.csv', 'mallorca_listings4_long.csv', '__MACOSX/._mallorca_listings4_long.csv', 'mallorca_listings4.csv', '__MACOSX/._mallorca_listings4.csv', 'mallorca_listings3_long.csv', '__MACOSX/._mallorca_listings3_long.csv', 'mallorca_listings3.csv', '__MACOSX/._mallorca_listings3.csv', 'mallorca_listings2_long.csv', '__MACOSX/._mallorca_listings2_long.csv', 'mallorca_listings2.csv', '__MACOSX/._mallorca_listings2.csv', 'mallorca_listings1_long.csv', '__MACOSX/._mallorca_listings1_long.csv', 'mallorca_listings1.csv', '__MACOSX/._mallorca_listings1.csv', 'calendar_mallorca4.csv', '__MACOSX/._calendar_mallorca4.csv', 'calendar_mallorca3.csv', '__MACOSX/._calendar_mallorca3.csv']\n",
            "Files for mallorca have been extracted to: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbeEjXi7orTK"
      },
      "source": [
        "###**Processing Calender Data**\n",
        "\n",
        "This function processes the calendar data for a city by:\n",
        "\n",
        "- **Combining Files**: Reads and merges multiple calendar CSV files.\n",
        "- **Filtering**: Removes unavailable listings and unnecessary columns.\n",
        "- **Date Handling**: Standardizes dates to the first of the month and removes invalid dates.\n",
        "- **Cleanup**: Sorts data, drops duplicates, and converts `listing_id` to integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_calender = process_city_calender(city_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDjx3DERQ1DH",
        "outputId": "8ddcab7c-80c9-4f70-c343-3f2dbd2e4c82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in calendar_mallorca1.csv: 6283421\n",
            "Rows in calendar_mallorca2.csv: 6094770\n",
            "Rows in calendar_mallorca3.csv: 6613187\n",
            "Rows in calendar_mallorca4.csv: 6877031\n",
            "Initial combined rows: 25868409\n",
            "Rows after filtering 'available' column: 13723339\n",
            "Number of rows with invalid dates: 0\n",
            "Rows after dropping invalid dates: 13723339\n",
            "Rows after dropping duplicates: 321679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z75czsHuo3IE"
      },
      "source": [
        "###**Processing Listings Data**\n",
        "\n",
        "This function processes the listings data for a city by:\n",
        "\n",
        "- **Loading and Combining**: Merges multiple listings CSV files.\n",
        "- **Cleaning**: Removes duplicates and unnecessary columns.\n",
        "- **Merging**: Combines additional details from long-format listings.\n",
        "- **Imputation**: Fills missing `review_scores_rating` with the mean and populates empty descriptions.\n",
        "- **Final Preparation**: Ensures correct data types for further analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the process\n",
        "combined_listings_extended = process_city_listings(city_name)"
      ],
      "metadata": {
        "id": "xZ2oHrHhY2hm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqPCcu89pP6o"
      },
      "source": [
        "###**Merging Listings and Calender Data**\n",
        "\n",
        "\n",
        "This function merges calendar and listings data, cleans it, and prepares it for analysis:\n",
        "\n",
        "- **Renaming and Merging**: Renames columns and merges datasets on `id`.\n",
        "- **Cleaning**: Removes unnecessary columns, cleans price data, and handles missing values.\n",
        "- **Filtering**: Filters the data by date range.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the process\n",
        "combined_data = prepare_combined_data(combined_calender, combined_listings_extended)"
      ],
      "metadata": {
        "id": "zvNuo8yrZbFC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyijjX4FpT-J"
      },
      "source": [
        "###**NLP Metric-1 #Zero-Shot Classification for Categorizing Descriptions**\n",
        "\n",
        "This section classifies property descriptions into categories using a zero-shot classification model:\n",
        "\n",
        "- **Setup**: Initializes the classifier and prepares the data.\n",
        "- **Classification**: Categorizes descriptions as \"Luxury,\" \"Standard,\" or \"Economy\" in batches.\n",
        "- **Merge**: Adds the classification results back into the combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "combined_data = classify_property_descriptions(combined_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4qTbV6oZe3A",
        "outputId": "f6eccfc3-0b58-49df-c100-7593619fefa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Processing Batches:  92%|█████████▏| 335/366 [18:39<01:46,  3.45s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWm2xqzpptyw"
      },
      "source": [
        "###**NLP Metric-2 # Processing and Analyzing Amenities Data**\n",
        "\n",
        "This function processes the amenities data for each property and calculates the most common amenities by neighborhood:\n",
        "\n",
        "- **Amenity Handling**: Cleans and formats the `amenities` column into a list.\n",
        "- **Top Amenities**: Identifies the top amenities for each neighborhood, including their frequency and percentage.\n",
        "- **Merge**: Adds this information back into the main dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the process\n",
        "combined_data = process_combined_data(combined_data)"
      ],
      "metadata": {
        "id": "1l5c71etZ_b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1jClj9p2qy"
      },
      "source": [
        "###**NLP Metric-3 # Sentiment Analaysis on Descriptions of Listings**\n",
        "\n",
        "This section applies sentiment analysis to property descriptions:\n",
        "\n",
        "- **Model Initialization**: Uses a multilingual BERT model for sentiment analysis.\n",
        "- **Text Splitting**: Splits longer texts to fit the model's input size.\n",
        "- **Batch Processing**: Analyzes sentiment in batches for efficiency.\n",
        "- **Result Handling**: Extracts sentiment labels and scores, adding them to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the function\n",
        "combined_data = analyze_sentiment(combined_data)"
      ],
      "metadata": {
        "id": "vEqCsN3iaCZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJEkBxdKqZbo"
      },
      "source": [
        "###**NLP Metric-4 Sentiment Analysis on Rewievs**\n",
        "\n",
        "\n",
        "\n",
        "This function processes review data for a city, performs sentiment analysis, and merges the results into the main dataset:\n",
        "\n",
        "- **Data Loading**: Loads and optimizes review data for memory efficiency.\n",
        "- **Filtering**: Focuses on reviews within a specified date range and linked to existing listings.\n",
        "- **Sentiment Analysis**: Uses a multilingual BERT model to assess review sentiment.\n",
        "- **Scoring**: Calculates average sentiment scores and assigns a star rating based on these scores.\n",
        "- **Merging**: Integrates the sentiment scores and star ratings into the main dataset.\n",
        "\n",
        "**Note**: This metric was not included in the dashboard due to RAM and computational limitations, but it is recommended to try generating it since the review data is already loaded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psbtF4sA2Ix7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run the function\n",
        "#combined_data = pd.read_csv('/content/florence_final_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ARIMA Forecasting for Neighborhood Prices**\n",
        "\n",
        "This function performs ARIMA-based forecasting on average neighborhood prices:\n",
        "\n",
        "- **Grid Search**: Searches for the best ARIMA model parameters to fit the data.\n",
        "- **Forecasting**: Predicts future prices for each neighborhood based on historical data.\n",
        "- **Handling Insufficient Data**: Skips neighborhoods with inadequate data.\n",
        "- **Merging Results**: Combines forecasted data with the original dataset for comprehensive analysis.\n"
      ],
      "metadata": {
        "id": "D9sQJhNL4-F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the function\n",
        "combined_data = arima_forecast_and_save(city_name, combined_data)"
      ],
      "metadata": {
        "id": "vGwXhSAzaQww"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}